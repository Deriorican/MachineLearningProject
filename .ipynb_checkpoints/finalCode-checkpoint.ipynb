{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project in Mahine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3344525334\n"
     ]
    }
   ],
   "source": [
    "from utility import *\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "mySeed = randint(0, 2**32 - 1)\n",
    "print(mySeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Seed to use to compare all different models is 3496236729."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    X1, X2, Y1 = open_data(\"C:\\\\Users\\\\Louis Lovat\\\\Desktop\\\\UNIF\\\\MachineLearning\\\\Project\\\\data\")\n",
    "except :\n",
    "    X1, X2, Y1 = open_data(\"C:\\\\Users\\\\hchri\\\\Jupyter\\\\Machine learning\\\\project_data\")\n",
    "    \n",
    "X2.drop([\"title\", \"is_adult\", \"img_url\", \"description\", \"Unnamed: 0\"], axis=1, inplace=True)\n",
    "X1_p = X1.drop([\"title\", \"is_adult\", \"img_url\", \"description\", \"Unnamed: 0\"], axis=1)\n",
    "colmns = ['ratings','n_votes','production_year','runtime','release_year']\n",
    "\n",
    "# General preprocessing\n",
    "\n",
    "X1_p = Binarize(X1_p, 'studio', 10)\n",
    "X1_p = Binarize(X1_p, 'genres', 5)\n",
    "\n",
    "X1_p = strToFloatArray(X1_p, \"text_embeddings\")\n",
    "X1_p = strToFloatArray(X1_p, \"img_embeddings\")\n",
    "\n",
    "mySeed = 3496236729\n",
    "otherSeed = 3943876323\n",
    "n_model_folds = 10\n",
    "n_parameters_folds = 10\n",
    "\n",
    "\"\"\"\n",
    "UltimeglobalProcess = Process(X1_p, Y1)\n",
    "UltimeglobalProcess.setTrainTest()\n",
    "globalProcess = Process(UltimeglobalProcess.X_train, UltimeglobalProcess.Y_train)\n",
    "\"\"\"\n",
    "\n",
    "globalProcess = Process(X1_p, Y1)\n",
    "KF = KFold(n_splits=n_model_folds, shuffle=True, random_state=mySeed)\n",
    "modelFolds = []; modelTrainIndices = []; modelTestIndices = []\n",
    "for i, (train_index, test_index) in enumerate(KF.split(globalProcess.X)):\n",
    "    modelFolds.append(i)\n",
    "    modelTrainIndices.append(train_index)\n",
    "    modelTestIndices.append(test_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings</th>\n",
       "      <th>n_votes</th>\n",
       "      <th>production_year</th>\n",
       "      <th>runtime</th>\n",
       "      <th>release_year</th>\n",
       "      <th>studio_7Art</th>\n",
       "      <th>studio_AL</th>\n",
       "      <th>studio_Abr.</th>\n",
       "      <th>studio_Anch.</th>\n",
       "      <th>studio_Art.</th>\n",
       "      <th>...</th>\n",
       "      <th>img_embeddings90</th>\n",
       "      <th>img_embeddings91</th>\n",
       "      <th>img_embeddings92</th>\n",
       "      <th>img_embeddings93</th>\n",
       "      <th>img_embeddings94</th>\n",
       "      <th>img_embeddings95</th>\n",
       "      <th>img_embeddings96</th>\n",
       "      <th>img_embeddings97</th>\n",
       "      <th>img_embeddings98</th>\n",
       "      <th>img_embeddings99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>6.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2001</td>\n",
       "      <td>103.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.269453</td>\n",
       "      <td>3.764313</td>\n",
       "      <td>1.439066</td>\n",
       "      <td>-3.085937</td>\n",
       "      <td>0.386472</td>\n",
       "      <td>-0.884048</td>\n",
       "      <td>0.352875</td>\n",
       "      <td>2.057364</td>\n",
       "      <td>0.632560</td>\n",
       "      <td>-3.758074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>4.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.650711</td>\n",
       "      <td>-0.008627</td>\n",
       "      <td>1.125247</td>\n",
       "      <td>-3.098997</td>\n",
       "      <td>-0.498233</td>\n",
       "      <td>2.056440</td>\n",
       "      <td>-1.042187</td>\n",
       "      <td>-2.951290</td>\n",
       "      <td>0.379047</td>\n",
       "      <td>-0.935647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>5.9</td>\n",
       "      <td>8072.0</td>\n",
       "      <td>1983</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.677746</td>\n",
       "      <td>0.578083</td>\n",
       "      <td>-3.450681</td>\n",
       "      <td>-1.499255</td>\n",
       "      <td>3.522215</td>\n",
       "      <td>-0.809277</td>\n",
       "      <td>0.142012</td>\n",
       "      <td>-0.575228</td>\n",
       "      <td>-4.107932</td>\n",
       "      <td>-3.061923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2319</th>\n",
       "      <td>6.2</td>\n",
       "      <td>219.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.225866</td>\n",
       "      <td>0.181619</td>\n",
       "      <td>-2.451521</td>\n",
       "      <td>0.575630</td>\n",
       "      <td>-1.675617</td>\n",
       "      <td>3.623347</td>\n",
       "      <td>-2.442856</td>\n",
       "      <td>2.425007</td>\n",
       "      <td>-1.094412</td>\n",
       "      <td>-1.502455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>6.6</td>\n",
       "      <td>1188.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>108.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.235462</td>\n",
       "      <td>1.857578</td>\n",
       "      <td>9.597026</td>\n",
       "      <td>-0.657131</td>\n",
       "      <td>3.544536</td>\n",
       "      <td>2.281035</td>\n",
       "      <td>-2.251976</td>\n",
       "      <td>0.098857</td>\n",
       "      <td>-2.084741</td>\n",
       "      <td>1.339892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>4.4</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1971</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.383604</td>\n",
       "      <td>4.995472</td>\n",
       "      <td>2.288302</td>\n",
       "      <td>1.477090</td>\n",
       "      <td>-3.264078</td>\n",
       "      <td>-3.505116</td>\n",
       "      <td>-0.025688</td>\n",
       "      <td>1.723924</td>\n",
       "      <td>-2.455163</td>\n",
       "      <td>0.494622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>6.8</td>\n",
       "      <td>369.0</td>\n",
       "      <td>1996</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.766962</td>\n",
       "      <td>-0.219782</td>\n",
       "      <td>-2.566915</td>\n",
       "      <td>1.311991</td>\n",
       "      <td>1.661288</td>\n",
       "      <td>-2.393270</td>\n",
       "      <td>0.547398</td>\n",
       "      <td>0.419973</td>\n",
       "      <td>-2.825703</td>\n",
       "      <td>-1.119900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>4.4</td>\n",
       "      <td>355.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>124.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.959732</td>\n",
       "      <td>1.678894</td>\n",
       "      <td>1.664043</td>\n",
       "      <td>-1.579479</td>\n",
       "      <td>0.126228</td>\n",
       "      <td>1.463178</td>\n",
       "      <td>2.828866</td>\n",
       "      <td>-0.051471</td>\n",
       "      <td>-0.010896</td>\n",
       "      <td>-0.107656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2186</th>\n",
       "      <td>6.2</td>\n",
       "      <td>8294.0</td>\n",
       "      <td>1997</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.321058</td>\n",
       "      <td>-4.506417</td>\n",
       "      <td>2.519623</td>\n",
       "      <td>-1.848668</td>\n",
       "      <td>-1.771518</td>\n",
       "      <td>2.873996</td>\n",
       "      <td>-0.379668</td>\n",
       "      <td>-2.617757</td>\n",
       "      <td>-1.921466</td>\n",
       "      <td>-1.552974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1455.0</td>\n",
       "      <td>1997</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.389215</td>\n",
       "      <td>-2.411840</td>\n",
       "      <td>-0.600103</td>\n",
       "      <td>-0.639020</td>\n",
       "      <td>1.271825</td>\n",
       "      <td>0.569608</td>\n",
       "      <td>1.259971</td>\n",
       "      <td>-1.049056</td>\n",
       "      <td>-1.062121</td>\n",
       "      <td>-0.851509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2360 rows × 295 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ratings  n_votes  production_year  runtime  release_year  studio_7Art  \\\n",
       "552       6.0    322.0             2001    103.0        2003.0            0   \n",
       "2976      4.0    560.0             2002     70.0        2003.0            0   \n",
       "585       5.9   8072.0             1983    103.0        1983.0            0   \n",
       "2319      6.2    219.0             2000     96.0        2012.0            0   \n",
       "2024      6.6   1188.0             2011    108.0        2011.0            0   \n",
       "...       ...      ...              ...      ...           ...          ...   \n",
       "1335      4.4    166.0             1971     94.0        1992.0            0   \n",
       "804       6.8    369.0             1996     98.0        1998.0            0   \n",
       "513       4.4    355.0             2000    124.0        2000.0            0   \n",
       "2186      6.2   8294.0             1997    166.0        1997.0            0   \n",
       "444       6.0   1455.0             1997     84.0        1997.0            0   \n",
       "\n",
       "      studio_AL  studio_Abr.  studio_Anch.  studio_Art.  ...  \\\n",
       "552           0            0             0            0  ...   \n",
       "2976          0            0             0            0  ...   \n",
       "585           0            0             0            0  ...   \n",
       "2319          0            0             0            0  ...   \n",
       "2024          0            0             0            0  ...   \n",
       "...         ...          ...           ...          ...  ...   \n",
       "1335          0            0             0            0  ...   \n",
       "804           0            0             0            0  ...   \n",
       "513           0            0             0            0  ...   \n",
       "2186          0            0             0            0  ...   \n",
       "444           0            0             0            0  ...   \n",
       "\n",
       "      img_embeddings90  img_embeddings91  img_embeddings92  img_embeddings93  \\\n",
       "552           2.269453          3.764313          1.439066         -3.085937   \n",
       "2976         -1.650711         -0.008627          1.125247         -3.098997   \n",
       "585           2.677746          0.578083         -3.450681         -1.499255   \n",
       "2319         -4.225866          0.181619         -2.451521          0.575630   \n",
       "2024         -1.235462          1.857578          9.597026         -0.657131   \n",
       "...                ...               ...               ...               ...   \n",
       "1335          2.383604          4.995472          2.288302          1.477090   \n",
       "804          -0.766962         -0.219782         -2.566915          1.311991   \n",
       "513          -2.959732          1.678894          1.664043         -1.579479   \n",
       "2186         -1.321058         -4.506417          2.519623         -1.848668   \n",
       "444          -1.389215         -2.411840         -0.600103         -0.639020   \n",
       "\n",
       "      img_embeddings94  img_embeddings95  img_embeddings96  img_embeddings97  \\\n",
       "552           0.386472         -0.884048          0.352875          2.057364   \n",
       "2976         -0.498233          2.056440         -1.042187         -2.951290   \n",
       "585           3.522215         -0.809277          0.142012         -0.575228   \n",
       "2319         -1.675617          3.623347         -2.442856          2.425007   \n",
       "2024          3.544536          2.281035         -2.251976          0.098857   \n",
       "...                ...               ...               ...               ...   \n",
       "1335         -3.264078         -3.505116         -0.025688          1.723924   \n",
       "804           1.661288         -2.393270          0.547398          0.419973   \n",
       "513           0.126228          1.463178          2.828866         -0.051471   \n",
       "2186         -1.771518          2.873996         -0.379668         -2.617757   \n",
       "444           1.271825          0.569608          1.259971         -1.049056   \n",
       "\n",
       "      img_embeddings98  img_embeddings99  \n",
       "552           0.632560         -3.758074  \n",
       "2976          0.379047         -0.935647  \n",
       "585          -4.107932         -3.061923  \n",
       "2319         -1.094412         -1.502455  \n",
       "2024         -2.084741          1.339892  \n",
       "...                ...               ...  \n",
       "1335         -2.455163          0.494622  \n",
       "804          -2.825703         -1.119900  \n",
       "513          -0.010896         -0.107656  \n",
       "2186         -1.921466         -1.552974  \n",
       "444          -1.062121         -0.851509  \n",
       "\n",
       "[2360 rows x 295 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_votes</th>\n",
       "      <th>release_year</th>\n",
       "      <th>production_year</th>\n",
       "      <th>img_embeddings5</th>\n",
       "      <th>runtime</th>\n",
       "      <th>ratings</th>\n",
       "      <th>img_embeddings13</th>\n",
       "      <th>text_embeddings32</th>\n",
       "      <th>img_embeddings28</th>\n",
       "      <th>img_embeddings49</th>\n",
       "      <th>...</th>\n",
       "      <th>img_embeddings48</th>\n",
       "      <th>genres_Adventure</th>\n",
       "      <th>text_embeddings64</th>\n",
       "      <th>text_embeddings5</th>\n",
       "      <th>img_embeddings81</th>\n",
       "      <th>text_embeddings46</th>\n",
       "      <th>text_embeddings42</th>\n",
       "      <th>img_embeddings82</th>\n",
       "      <th>text_embeddings66</th>\n",
       "      <th>img_embeddings14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>322.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>2001</td>\n",
       "      <td>3.752244</td>\n",
       "      <td>103.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-7.407696</td>\n",
       "      <td>1.964302</td>\n",
       "      <td>3.371798</td>\n",
       "      <td>-4.824652</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.731260</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.562496</td>\n",
       "      <td>0.301131</td>\n",
       "      <td>-0.439618</td>\n",
       "      <td>0.393409</td>\n",
       "      <td>-0.324636</td>\n",
       "      <td>2.656731</td>\n",
       "      <td>0.004473</td>\n",
       "      <td>-0.451701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>560.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>2.028882</td>\n",
       "      <td>70.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-2.667827</td>\n",
       "      <td>0.911263</td>\n",
       "      <td>4.121628</td>\n",
       "      <td>-5.564388</td>\n",
       "      <td>...</td>\n",
       "      <td>4.055173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.162049</td>\n",
       "      <td>2.199155</td>\n",
       "      <td>0.625331</td>\n",
       "      <td>0.930649</td>\n",
       "      <td>-0.385919</td>\n",
       "      <td>-2.232609</td>\n",
       "      <td>-0.105128</td>\n",
       "      <td>-2.875535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>8072.0</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>1983</td>\n",
       "      <td>-5.131067</td>\n",
       "      <td>103.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>-1.352164</td>\n",
       "      <td>0.269130</td>\n",
       "      <td>-1.328011</td>\n",
       "      <td>-3.525231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865788</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.055071</td>\n",
       "      <td>-1.775514</td>\n",
       "      <td>-3.426266</td>\n",
       "      <td>0.839558</td>\n",
       "      <td>1.013227</td>\n",
       "      <td>0.052752</td>\n",
       "      <td>0.064517</td>\n",
       "      <td>-2.370932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2319</th>\n",
       "      <td>219.0</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>10.339752</td>\n",
       "      <td>96.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>-4.213018</td>\n",
       "      <td>-0.156439</td>\n",
       "      <td>-4.156349</td>\n",
       "      <td>-1.796714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098288</td>\n",
       "      <td>0</td>\n",
       "      <td>0.815635</td>\n",
       "      <td>3.139698</td>\n",
       "      <td>3.570636</td>\n",
       "      <td>0.164989</td>\n",
       "      <td>0.385809</td>\n",
       "      <td>-1.291655</td>\n",
       "      <td>0.770880</td>\n",
       "      <td>0.532061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>1188.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>-2.569890</td>\n",
       "      <td>108.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>-2.956260</td>\n",
       "      <td>0.586414</td>\n",
       "      <td>0.178132</td>\n",
       "      <td>-0.233733</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.072709</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.004713</td>\n",
       "      <td>3.152512</td>\n",
       "      <td>-2.252355</td>\n",
       "      <td>-0.122467</td>\n",
       "      <td>-0.387008</td>\n",
       "      <td>-1.163259</td>\n",
       "      <td>0.017794</td>\n",
       "      <td>1.536630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>166.0</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>1971</td>\n",
       "      <td>15.433016</td>\n",
       "      <td>94.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>-6.126474</td>\n",
       "      <td>0.068362</td>\n",
       "      <td>6.406932</td>\n",
       "      <td>-4.638857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218826</td>\n",
       "      <td>0</td>\n",
       "      <td>0.168872</td>\n",
       "      <td>-2.494770</td>\n",
       "      <td>-1.413839</td>\n",
       "      <td>-0.165288</td>\n",
       "      <td>-0.234820</td>\n",
       "      <td>1.291897</td>\n",
       "      <td>-0.119379</td>\n",
       "      <td>-6.022030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>369.0</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>1996</td>\n",
       "      <td>8.857011</td>\n",
       "      <td>98.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>-3.954752</td>\n",
       "      <td>-0.605992</td>\n",
       "      <td>5.089389</td>\n",
       "      <td>1.009288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.653906</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.717051</td>\n",
       "      <td>-2.397882</td>\n",
       "      <td>1.279601</td>\n",
       "      <td>-0.613854</td>\n",
       "      <td>-0.856652</td>\n",
       "      <td>-2.932577</td>\n",
       "      <td>0.110951</td>\n",
       "      <td>-7.367950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>355.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>7.506249</td>\n",
       "      <td>124.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.254145</td>\n",
       "      <td>0.909713</td>\n",
       "      <td>-0.995948</td>\n",
       "      <td>3.027209</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.484764</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.316023</td>\n",
       "      <td>-0.054709</td>\n",
       "      <td>-2.872057</td>\n",
       "      <td>-0.000298</td>\n",
       "      <td>0.852510</td>\n",
       "      <td>3.280136</td>\n",
       "      <td>-0.063494</td>\n",
       "      <td>1.415169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2186</th>\n",
       "      <td>8294.0</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>1997</td>\n",
       "      <td>4.494184</td>\n",
       "      <td>166.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.953767</td>\n",
       "      <td>-0.563397</td>\n",
       "      <td>0.223098</td>\n",
       "      <td>-3.310562</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.447090</td>\n",
       "      <td>0</td>\n",
       "      <td>0.213186</td>\n",
       "      <td>0.585461</td>\n",
       "      <td>-0.583713</td>\n",
       "      <td>0.311649</td>\n",
       "      <td>1.614494</td>\n",
       "      <td>-0.703115</td>\n",
       "      <td>-0.187180</td>\n",
       "      <td>-3.762640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>1455.0</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>1997</td>\n",
       "      <td>11.334975</td>\n",
       "      <td>84.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-8.801565</td>\n",
       "      <td>-0.773453</td>\n",
       "      <td>0.110906</td>\n",
       "      <td>-3.223790</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.963887</td>\n",
       "      <td>0</td>\n",
       "      <td>0.049949</td>\n",
       "      <td>2.374677</td>\n",
       "      <td>1.601153</td>\n",
       "      <td>-0.606666</td>\n",
       "      <td>-0.644094</td>\n",
       "      <td>1.174356</td>\n",
       "      <td>0.493022</td>\n",
       "      <td>2.608624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2360 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      n_votes  release_year  production_year  img_embeddings5  runtime  \\\n",
       "552     322.0        2003.0             2001         3.752244    103.0   \n",
       "2976    560.0        2003.0             2002         2.028882     70.0   \n",
       "585    8072.0        1983.0             1983        -5.131067    103.0   \n",
       "2319    219.0        2012.0             2000        10.339752     96.0   \n",
       "2024   1188.0        2011.0             2011        -2.569890    108.0   \n",
       "...       ...           ...              ...              ...      ...   \n",
       "1335    166.0        1992.0             1971        15.433016     94.0   \n",
       "804     369.0        1998.0             1996         8.857011     98.0   \n",
       "513     355.0        2000.0             2000         7.506249    124.0   \n",
       "2186   8294.0        1997.0             1997         4.494184    166.0   \n",
       "444    1455.0        1997.0             1997        11.334975     84.0   \n",
       "\n",
       "      ratings  img_embeddings13  text_embeddings32  img_embeddings28  \\\n",
       "552       6.0         -7.407696           1.964302          3.371798   \n",
       "2976      4.0         -2.667827           0.911263          4.121628   \n",
       "585       5.9         -1.352164           0.269130         -1.328011   \n",
       "2319      6.2         -4.213018          -0.156439         -4.156349   \n",
       "2024      6.6         -2.956260           0.586414          0.178132   \n",
       "...       ...               ...                ...               ...   \n",
       "1335      4.4         -6.126474           0.068362          6.406932   \n",
       "804       6.8         -3.954752          -0.605992          5.089389   \n",
       "513       4.4          0.254145           0.909713         -0.995948   \n",
       "2186      6.2          0.953767          -0.563397          0.223098   \n",
       "444       6.0         -8.801565          -0.773453          0.110906   \n",
       "\n",
       "      img_embeddings49  ...  img_embeddings48  genres_Adventure  \\\n",
       "552          -4.824652  ...         -4.731260                 0   \n",
       "2976         -5.564388  ...          4.055173                 0   \n",
       "585          -3.525231  ...          0.865788                 0   \n",
       "2319         -1.796714  ...          0.098288                 0   \n",
       "2024         -0.233733  ...         -4.072709                 0   \n",
       "...                ...  ...               ...               ...   \n",
       "1335         -4.638857  ...          0.218826                 0   \n",
       "804           1.009288  ...          0.653906                 0   \n",
       "513           3.027209  ...         -2.484764                 0   \n",
       "2186         -3.310562  ...         -5.447090                 0   \n",
       "444          -3.223790  ...         -5.963887                 0   \n",
       "\n",
       "      text_embeddings64  text_embeddings5  img_embeddings81  \\\n",
       "552           -0.562496          0.301131         -0.439618   \n",
       "2976           0.162049          2.199155          0.625331   \n",
       "585           -0.055071         -1.775514         -3.426266   \n",
       "2319           0.815635          3.139698          3.570636   \n",
       "2024          -0.004713          3.152512         -2.252355   \n",
       "...                 ...               ...               ...   \n",
       "1335           0.168872         -2.494770         -1.413839   \n",
       "804           -0.717051         -2.397882          1.279601   \n",
       "513           -0.316023         -0.054709         -2.872057   \n",
       "2186           0.213186          0.585461         -0.583713   \n",
       "444            0.049949          2.374677          1.601153   \n",
       "\n",
       "      text_embeddings46  text_embeddings42  img_embeddings82  \\\n",
       "552            0.393409          -0.324636          2.656731   \n",
       "2976           0.930649          -0.385919         -2.232609   \n",
       "585            0.839558           1.013227          0.052752   \n",
       "2319           0.164989           0.385809         -1.291655   \n",
       "2024          -0.122467          -0.387008         -1.163259   \n",
       "...                 ...                ...               ...   \n",
       "1335          -0.165288          -0.234820          1.291897   \n",
       "804           -0.613854          -0.856652         -2.932577   \n",
       "513           -0.000298           0.852510          3.280136   \n",
       "2186           0.311649           1.614494         -0.703115   \n",
       "444           -0.606666          -0.644094          1.174356   \n",
       "\n",
       "      text_embeddings66  img_embeddings14  \n",
       "552            0.004473         -0.451701  \n",
       "2976          -0.105128         -2.875535  \n",
       "585            0.064517         -2.370932  \n",
       "2319           0.770880          0.532061  \n",
       "2024           0.017794          1.536630  \n",
       "...                 ...               ...  \n",
       "1335          -0.119379         -6.022030  \n",
       "804            0.110951         -7.367950  \n",
       "513           -0.063494          1.415169  \n",
       "2186          -0.187180         -3.762640  \n",
       "444            0.493022          2.608624  \n",
       "\n",
       "[2360 rows x 100 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display(X1_p)\n",
    "testProcess = Process(X1_p, Y1)\n",
    "testProcess.setTrainTest()\n",
    "testProcess.X_train.fillna(testProcess.X_train[\"runtime\"].mean(), inplace=True)\n",
    "testProcess.X_test.fillna(testProcess.X_train[\"runtime\"].mean(), inplace=True)\n",
    "#globalProcess.removeDuplicate(colmns)   \n",
    "testProcess.PCA_embeddings(\"text_embeddings\", 100)\n",
    "testProcess.PCA_embeddings(\"img_embeddings\", 100)\n",
    "    \n",
    "display(testProcess.X_train)\n",
    "testProcess.MI_selection(100, worst=False)\n",
    "display(testProcess.X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "Running Fold 0 for model Selection : linear\n",
      "===========================================\n",
      "295\n",
      "17\n",
      "In these conditions RMSE value on this iteration is : 37.63544554895618\n",
      "\n",
      "===========================================\n",
      "Running Fold 1 for model Selection : linear\n",
      "===========================================\n",
      "295\n",
      "14\n",
      "In these conditions RMSE value on this iteration is : 64.1328697345699\n",
      "\n",
      "===========================================\n",
      "Running Fold 2 for model Selection : linear\n",
      "===========================================\n",
      "295\n",
      "20\n",
      "In these conditions RMSE value on this iteration is : 43.371191222727326\n",
      "\n",
      "===========================================\n",
      "Running Fold 3 for model Selection : linear\n",
      "===========================================\n",
      "295\n",
      "14\n",
      "In these conditions RMSE value on this iteration is : 72.01429447116858\n",
      "\n",
      "===========================================\n",
      "Running Fold 4 for model Selection : linear\n",
      "===========================================\n",
      "295\n",
      "15\n",
      "In these conditions RMSE value on this iteration is : 72.26350025631734\n",
      "\n",
      "===========================================\n",
      "Running Fold 5 for model Selection : linear\n",
      "===========================================\n",
      "295\n",
      "17\n",
      "In these conditions RMSE value on this iteration is : 66.37182727945564\n",
      "\n",
      "===========================================\n",
      "Running Fold 6 for model Selection : linear\n",
      "===========================================\n",
      "295\n",
      "18\n",
      "In these conditions RMSE value on this iteration is : 51.92799745141705\n",
      "\n",
      "===========================================\n",
      "Running Fold 7 for model Selection : linear\n",
      "===========================================\n",
      "295\n",
      "15\n",
      "In these conditions RMSE value on this iteration is : 58.13764152016699\n",
      "\n",
      "===========================================\n",
      "Running Fold 8 for model Selection : linear\n",
      "===========================================\n",
      "295\n",
      "18\n",
      "In these conditions RMSE value on this iteration is : 51.99006283784255\n",
      "\n",
      "===========================================\n",
      "Running Fold 9 for model Selection : linear\n",
      "===========================================\n",
      "295\n",
      "17\n",
      "In these conditions RMSE value on this iteration is : 53.19249589303915\n",
      "\n",
      "The mean RMSE for the linear model in best conditions is 57.10373262156607\n"
     ]
    }
   ],
   "source": [
    "modelName = \"linear\"\n",
    "modelResults = np.zeros(n_model_folds)\n",
    "for i, train_index, test_index in zip(modelFolds, modelTrainIndices, modelTestIndices):\n",
    "    \n",
    "    # The purpose of this loop is to compute the Cross-Validation score of the linear model based on the RMSE\n",
    "    # The score of the different models will be used to choose which model is the best suited for our project\n",
    "    # Each model will perform in the best conditions (the best parameters are selected for the model)\n",
    "\n",
    "    # Setting up the train and test sets\n",
    "\n",
    "    toPrint = f\"Running Fold {i} for model Selection : {modelName}\"\n",
    "    print(\"=\"*len(toPrint) + \"\\n\" +  toPrint + \"\\n\" + \"=\"*len(toPrint))\n",
    "    globalProcess.setTrainTest(train_index = train_index, test_index = test_index)\n",
    "\n",
    "    # Preprocessing\n",
    "\n",
    "    # Replacing missing values by the mean\n",
    "    globalProcess.X_train.fillna(globalProcess.X_train[\"runtime\"].mean(), inplace=True)\n",
    "    globalProcess.X_test.fillna(globalProcess.X_train[\"runtime\"].mean(), inplace=True)\n",
    "\n",
    "    # Removing duplicated data from the training set\n",
    "    globalProcess.removeDuplicate(colmns)\n",
    "    \n",
    "    # Selecting the most relevant components of the embeddings with PCA\n",
    "    globalProcess.PCA_embeddings(\"text_embeddings\", 100)\n",
    "    globalProcess.PCA_embeddings(\"img_embeddings\", 100)\n",
    "    \n",
    "    # Keeping only the features which are correlated with the target \n",
    "    # Correlation threshold = 0.1\n",
    "    #print(len(globalProcess.X_train.columns))\n",
    "    globalProcess.corrThreshold()\n",
    "    #print(len(globalProcess.X_train.columns))\n",
    "    globalProcess.addModel(modelName)\n",
    "    \n",
    "\n",
    "\n",
    "    pred = globalProcess.useModel(modelName)\n",
    "    pred2 = globalProcess.useModel(modelName, X = globalProcess.X_train)\n",
    "    modelResults[i] = compute_rmse(pred, globalProcess.Y_test*1e-6)\n",
    "    #modelResults2 = compute_rmse(pred2, globalProcess.Y_train*1e-6)\n",
    "    print(f\"In these conditions RMSE value on this iteration is : {modelResults[i]}\\n\")\n",
    "    #print(f\"In these conditions RMSE_train value on this iteration is : {modelResults2}\\n\")\n",
    "\n",
    "meanModelResult = np.mean(modelResults)\n",
    "print(f\"The mean RMSE for the {modelName} model in best conditions is {meanModelResult}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"linear\"\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "# Replacing missing values by the mean\n",
    "UltimeglobalProcess.X_train.fillna(UltimeglobalProcess.X_train[\"runtime\"].mean(), inplace=True)\n",
    "UltimeglobalProcess.X_test.fillna(UltimeglobalProcess.X_train[\"runtime\"].mean(), inplace=True)\n",
    "\n",
    "# Removing duplicated data from the training set\n",
    "UltimeglobalProcess.removeDuplicate(colmns)\n",
    "\n",
    "# Selecting the most relevant components of the embeddings with PCA\n",
    "UltimeglobalProcess.PCA_embeddings(\"text_embeddings\", 100)\n",
    "UltimeglobalProcess.PCA_embeddings(\"img_embeddings\", 100)\n",
    "\n",
    "# Keeping only the features which are correlated with the target \n",
    "# Correlation threshold = 0.1\n",
    "UltimeglobalProcess.corrThreshold()\n",
    "\n",
    "UltimeglobalProcess.addModel(modelName)\n",
    "pred = UltimeglobalProcess.useModel(modelName)\n",
    "print(compute_rmse(pred, UltimeglobalProcess.Y_test*1e-6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "allParameterSets = [(i, p, 'uniform') for i in range(1, 31) for p in range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Running Fold 0 for model Selection : knn\n",
      "========================================\n",
      "\tRunning Fold 0 for parameters' Selection : knn\n",
      "\tRunning Fold 1 for parameters' Selection : knn\n",
      "\tRunning Fold 2 for parameters' Selection : knn\n",
      "\tRunning Fold 3 for parameters' Selection : knn\n",
      "\tRunning Fold 4 for parameters' Selection : knn\n",
      "\tRunning Fold 5 for parameters' Selection : knn\n",
      "\tRunning Fold 6 for parameters' Selection : knn\n",
      "\tRunning Fold 7 for parameters' Selection : knn\n",
      "\tRunning Fold 8 for parameters' Selection : knn\n",
      "\tRunning Fold 9 for parameters' Selection : knn\n",
      "\n",
      "Best parameters' values are : (14, 5, 'uniform')\n",
      "In these conditions RMSE value on this iteration is : 37.99061645218811\n",
      "\n",
      "========================================\n",
      "Running Fold 1 for model Selection : knn\n",
      "========================================\n",
      "\tRunning Fold 0 for parameters' Selection : knn\n",
      "\tRunning Fold 1 for parameters' Selection : knn\n"
     ]
    }
   ],
   "source": [
    "modelName = \"knn\"\n",
    "# allParameterSets = [] # shape (n_combinations, n_parameters_per_set), one set is [n_neighbors, p, weights]\n",
    "modelResults = np.zeros(n_model_folds)\n",
    "for i, train_index, test_index in zip(modelFolds, modelTrainIndices, modelTestIndices):\n",
    "    \n",
    "    # The purpose of this loop is to compute the Cross-Validation score of the knn model based on the RMSE\n",
    "    # The score of the different models will be used to choose which model is the best suited for our project\n",
    "    # Each model will perform in the best conditions (the best parameters are selected for the model)\n",
    "\n",
    "    # Setting up the train and test sets\n",
    "\n",
    "    toPrint = f\"Running Fold {i} for model Selection : {modelName}\"\n",
    "    print(\"=\"*len(toPrint) + \"\\n\" +  toPrint + \"\\n\" + \"=\"*len(toPrint))\n",
    "    globalProcess.setTrainTest(train_index = train_index, test_index = test_index)\n",
    "\n",
    "    # Splitting the training data into 10 K-Folds in order to select the best parameters\n",
    "\n",
    "    subProcess = Process(globalProcess.X_train, globalProcess.Y_train)\n",
    "    subKF = KFold(n_splits=n_parameters_folds, shuffle=True)\n",
    "\n",
    "    # Selection of the best parameters\n",
    "\n",
    "    allParametersResults = np.zeros((len(allParameterSets), n_parameters_folds))\n",
    "    for j, (sub_train_index, sub_test_index) in enumerate(subKF.split(subProcess.X)):\n",
    "\n",
    "        # The purpose of this loop is to select the best parameters for the knn model\n",
    "        # We compute the Cross-Validation score based on the RMSE for each set of parameters\n",
    "        # The set with the lowest CV-score (mean RMSE on all folds) is chosen as the best set\n",
    "         \n",
    "        print(f\"\\tRunning Fold {j} for parameters' Selection : {modelName}\")\n",
    "\n",
    "        # Setting up the train and test sets\n",
    "\n",
    "        subProcess.setTrainTest(train_index = sub_train_index, test_index = sub_test_index)\n",
    "\n",
    "        # Preprocessing\n",
    "\n",
    "        # Replacing missing values by the mean\n",
    "        subProcess.X_train.fillna(subProcess.X_train[\"runtime\"].mean(), inplace=True)\n",
    "        subProcess.X_test.fillna(subProcess.X_train[\"runtime\"].mean(), inplace=True)\n",
    "\n",
    "        # Removing duplicated data from the training set\n",
    "        subProcess.removeDuplicate(colmns)\n",
    "        \n",
    "        # Selecting the most relevant components of the embeddings with PCA\n",
    "        subProcess.PCA_embeddings(\"text_embeddings\", 100)\n",
    "        subProcess.PCA_embeddings(\"img_embeddings\", 100)\n",
    "\n",
    "        # Removing redundant features because of their correlation\n",
    "        # subProcess.removeRedundantFeatures(threshold = 0.8)\n",
    "\n",
    "        # We scale the data with minmax (we take as min/max the maximal non-outliers values)\n",
    "        subProcess.minmaxize(withOutliers = False)\n",
    "\n",
    "        # Keeping only the features which are correlated with the target \n",
    "        # Correlation threshold = 0.1\n",
    "        #subProcess.corrThreshold()\n",
    "        subProcess.MI_selection(40, worst=False)\n",
    "        \n",
    "\n",
    "        # Variations of the set of parameters\n",
    "\n",
    "        for index, parameterSet in zip(range(len(allParameterSets)), allParameterSets):\n",
    "\n",
    "            # Building and training the model\n",
    "\n",
    "            subProcess.addModel(modelName, n_neighbors = parameterSet[0], p = parameterSet[1], weights = parameterSet[2])\n",
    "\n",
    "            pred = subProcess.useModel(modelName)\n",
    "            # Computing RMSE\n",
    "\n",
    "            allParametersResults[index, j] = compute_rmse(pred, subProcess.Y_test*1e-6)\n",
    "\n",
    "    # Computing wich set of parameters is the best one\n",
    "\n",
    "    meanResults = np.mean(allParametersResults, axis = 1)\n",
    "    bestParams = allParameterSets[np.argmin(meanResults)]\n",
    "    print(f\"\\nBest parameters' values are : {bestParams}\")\n",
    "\n",
    "    # Estimation of the RMSE with optimal parameters\n",
    "\n",
    "    # Preprocessing\n",
    "\n",
    "    # Replacing missing values by the mean\n",
    "    globalProcess.X_train.fillna(globalProcess.X_train[\"runtime\"].mean(), inplace=True)\n",
    "    globalProcess.X_test.fillna(globalProcess.X_train[\"runtime\"].mean(), inplace=True)\n",
    "\n",
    "    # Removing duplicated data from the training set\n",
    "    globalProcess.removeDuplicate(colmns)\n",
    "    \n",
    "    # Selecting the most relevant components of the embeddings with PCA\n",
    "    globalProcess.PCA_embeddings(\"text_embeddings\", 100)\n",
    "    globalProcess.PCA_embeddings(\"img_embeddings\", 100)\n",
    "\n",
    "    # Removing redundant features because of their correlation\n",
    "    #globalProcess.removeRedundantFeatures(threshold = 0.8)\n",
    "\n",
    "    # We scale the data with minmax (we take as min/max the maximal non-outliers values)\n",
    "    globalProcess.minmaxize(withOutliers = False)\n",
    "\n",
    "    # Keeping only the features which are correlated with the target \n",
    "    # Correlation threshold = 0.1\n",
    "    #globalProcess.corrThreshold()\n",
    "    globalProcess.MI_selection(40, worst=False)\n",
    "\n",
    "    globalProcess.addModel(modelName,n_neighbors = bestParams[0], p = bestParams[1], weights = bestParams[2])\n",
    "    pred = globalProcess.useModel(modelName)\n",
    "    modelResults[i] = compute_rmse(pred, globalProcess.Y_test*1e-6)\n",
    "    print(f\"In these conditions RMSE value on this iteration is : {modelResults[i]}\\n\")\n",
    "\n",
    "meanModelResult = np.mean(modelResults)\n",
    "print(f\"The mean RMSE for the {modelName} model in best conditions is {meanModelResult}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allParameterSets = [(20*i, 10*j) for i in range(1,8) for j in range(1, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"randomForest\"\n",
    "allParameterSets = [] # shape (n_combinations, n_parameters_per_set), one set is [n_estimators, max_depth]\n",
    "modelResults = np.zeros(n_model_folds)\n",
    "for i, train_index, test_index in zip(modelFolds, modelTrainIndices, modelTestIndices):\n",
    "    \n",
    "    # The purpose of this loop is to compute the Cross-Validation score of the randomForest model based on the RMSE\n",
    "    # The score of the different models will be used to choose which model is the best suited for our project\n",
    "    # Each model will perform in the best conditions (the best parameters are selected for the model)\n",
    "\n",
    "    # Setting up the train and test sets\n",
    "\n",
    "    toPrint = f\"Running Fold {i} for model Selection : {modelName}\"\n",
    "    print(\"=\"*len(toPrint) + \"\\n\" +  toPrint + \"\\n\" + \"=\"*len(toPrint))\n",
    "    globalProcess.setTrainTest(train_index = train_index, test_index = test_index)\n",
    "\n",
    "    # Splitting the training data into 10 K-Folds in order to select the best parameters\n",
    "\n",
    "    subProcess = Process(globalProcess.X_train, globalProcess.Y_train)\n",
    "    subKF = KFold(n_splits=n_parameters_folds, shuffle=True)\n",
    "\n",
    "    # Selection of the best parameters\n",
    "\n",
    "    allParametersResults = np.zeros((len(allParameterSets), n_parameters_folds))\n",
    "    for j, (sub_train_index, sub_test_index) in enumerate(subKF.split(subProcess.X)):\n",
    "\n",
    "        # The purpose of this loop is to select the best parameters for the randomForest model\n",
    "        # We compute the Cross-Validation score based on the RMSE for each set of parameters\n",
    "        # The set with the lowest CV-score (mean RMSE on all folds) is chosen as the best set\n",
    "         \n",
    "        print(f\"\\tRunning Fold {j} for parameters' Selection : {modelName}\")\n",
    "\n",
    "        # Setting up the train and test sets\n",
    "\n",
    "        subProcess.setTrainTest(train_index = sub_train_index, test_index = sub_test_index)\n",
    "\n",
    "        # Preprocessing\n",
    "\n",
    "        # Replacing missing values by the mean\n",
    "        subProcess.X_train.fillna(subProcess.X_train[\"runtime\"].mean(), inplace=True)\n",
    "        subProcess.X_test.fillna(subProcess.X_train[\"runtime\"].mean(), inplace=True)\n",
    "        \n",
    "        # Removing duplicated data from the training set\n",
    "        subProcess.removeDuplicate(colmns)\n",
    "        \n",
    "        # Selecting the most relevant components of the embeddings with PCA\n",
    "        subProcess.PCA_embeddings(\"text_embeddings\", 100)\n",
    "        subProcess.PCA_embeddings(\"img_embeddings\", 100)\n",
    "\n",
    "        # Removing redundant features because of their correlation\n",
    "        #subProcess.removeRedundantFeatures(threshold = 0.8)\n",
    "\n",
    "        # We scale the data with minmax (we take as min/max the maximal non-outliers values)\n",
    "        subProcess.minmaxize(withOutliers = False)\n",
    "\n",
    "        # Keeping only the features which are correlated with the target \n",
    "        # Correlation threshold = 0.1\n",
    "        #subProcess.corrThreshold()\n",
    "        subProcess.MI_selection(40, worst=False)\n",
    "\n",
    "        # Variations of the set of parameters\n",
    "\n",
    "        for index, parameterSet in zip(range(len(allParameterSets)), allParameterSets):\n",
    "\n",
    "            # Building and training the model\n",
    "\n",
    "            subProcess.addModel(modelName)\n",
    "            pred = subProcess.useModel(modelName, n_estimators = parameterSet[0], max_depth = parameterSet[1])\n",
    "\n",
    "            # Computing RMSE\n",
    "\n",
    "            allParametersResults[index, j] = compute_rmse(pred, subProcess.Y_test*1e-6)\n",
    "\n",
    "    # Computing wich set of parameters is the best one\n",
    "\n",
    "    meanResults = np.mean(allParametersResults, axis = 1)\n",
    "    bestParams = allParameterSets[np.argmin(meanResults)]\n",
    "    print(f\"\\nBest parameters' values are : {bestParams}\")\n",
    "\n",
    "    # Estimation of the RMSE with optimal parameters\n",
    "\n",
    "    # Preprocessing\n",
    "\n",
    "    # Replacing missing values by the mean\n",
    "    globalProcess.X_train.fillna(globalProcess.X_train[\"runtime\"].mean(), inplace=True)\n",
    "    globalProcess.X_test.fillna(globalProcess.X_train[\"runtime\"].mean(), inplace=True)\n",
    "\n",
    "    # Removing duplicated data from the training set\n",
    "    globalProcess.removeDuplicate(colmns)\n",
    "    \n",
    "    # Selecting the most relevant components of the embeddings with PCA\n",
    "    globalProcess.PCA_embeddings(\"text_embeddings\", 100)\n",
    "    globalProcess.PCA_embeddings(\"img_embeddings\", 100)\n",
    "\n",
    "    # Removing redundant features because of their correlation\n",
    "    #globalProcess.removeRedundantFeatures(threshold = 0.8)\n",
    "\n",
    "    # We scale the data with minmax (we take as min/max the maximal non-outliers values)\n",
    "    globalProcess.minmaxize(withOutliers = False)\n",
    "\n",
    "    # Keeping only the features which are correlated with the target \n",
    "    # Correlation threshold = 0.1\n",
    "    #globalProcess.corrThreshold()\n",
    "    globalProcess.MI_selection(40, worst=False)\n",
    "\n",
    "    globalProcess.addModel(modelName)\n",
    "    pred = globalProcess.useModel(modelName, n_estimators = bestParams[0], max_depth = bestParams[1])\n",
    "    modelResults[i] = compute_rmse(pred, globalProcess.Y_test*1e-6)\n",
    "    print(f\"In these conditions RMSE value on this iteration is : {modelResults[i]}\\n\")\n",
    "\n",
    "meanModelResult = np.mean(modelResults)\n",
    "print(f\"The mean RMSE for the {modelName} model in best conditions is {meanModelResult}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allParameterSets = [([100 * j]*i, 'relu') for i in range(2,6) for j in range(1, 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modelName = \"mlp\"\n",
    "#allParameterSets = [] # shape (n_combinations, n_parameters_per_set), one set is [hidden_layer_sizes, activation]\n",
    "modelResults = np.zeros(n_model_folds)\n",
    "for i, train_index, test_index in zip(modelFolds, modelTrainIndices, modelTestIndices):\n",
    "    \n",
    "    # The purpose of this loop is to compute the Cross-Validation score of the mlp model based on the RMSE\n",
    "    # The score of the different models will be used to choose which model is the best suited for our project\n",
    "    # Each model will perform in the best conditions (the best parameters are selected for the model)\n",
    "\n",
    "    # Setting up the train and test sets\n",
    "\n",
    "    toPrint = f\"Running Fold {i} for model Selection : {modelName}\"\n",
    "    print(\"=\"*len(toPrint) + \"\\n\" +  toPrint + \"\\n\" + \"=\"*len(toPrint))\n",
    "    globalProcess.setTrainTest(train_index = train_index, test_index = test_index)\n",
    "\n",
    "    # Splitting the training data into 10 K-Folds in order to select the best parameters\n",
    "\n",
    "    subProcess = Process(globalProcess.X_train, globalProcess.Y_train)\n",
    "    subKF = KFold(n_splits=n_parameters_folds, shuffle=True)\n",
    "\n",
    "    # Selection of the best parameters\n",
    "\n",
    "    allParametersResults = np.zeros((len(allParameterSets), n_parameters_folds))\n",
    "    for j, (sub_train_index, sub_test_index) in enumerate(subKF.split(subProcess.X)):\n",
    "\n",
    "        # The purpose of this loop is to select the best parameters for the mlp model\n",
    "        # We compute the Cross-Validation score based on the RMSE for each set of parameters\n",
    "        # The set with the lowest CV-score (mean RMSE on all folds) is chosen as the best set\n",
    "         \n",
    "        print(f\"\\tRunning Fold {j} for parameters' Selection : {modelName}\")\n",
    "\n",
    "        # Setting up the train and test sets\n",
    "\n",
    "        subProcess.setTrainTest(train_index = sub_train_index, test_index = sub_test_index)\n",
    "\n",
    "        # Preprocessing\n",
    "\n",
    "        # Replacing missing values by the mean\n",
    "        subProcess.X_train.fillna(subProcess.X_train[\"runtime\"].mean(), inplace=True)\n",
    "        subProcess.X_test.fillna(subProcess.X_train[\"runtime\"].mean(), inplace=True)\n",
    "\n",
    "        # Removing duplicated data from the training set\n",
    "        subProcess.removeDuplicate(colmns)\n",
    "        \n",
    "        # Selecting the most relevant components of the embeddings with PCA\n",
    "        subProcess.PCA_embeddings(\"text_embeddings\", 100)\n",
    "        subProcess.PCA_embeddings(\"img_embeddings\", 100)\n",
    "\n",
    "        # Removing redundant features because of their correlation\n",
    "        #subProcess.removeRedundantFeatures(threshold = 0.8)\n",
    "\n",
    "        # We scale the data with minmax (we take as min/max the maximal non-outliers values)\n",
    "        subProcess.minmaxize(withOutliers = False)\n",
    "\n",
    "        # Keeping only the features which are correlated with the target \n",
    "        # Correlation threshold = 0.1\n",
    "        #subProcess.corrThreshold()\n",
    "        subProcess.MI_selection(40, worst=False)\n",
    "\n",
    "        # Variations of the set of parameters\n",
    "\n",
    "        for index, parameterSet in zip(range(len(allParameterSets)), allParameterSets):\n",
    "\n",
    "            # Building and training the model\n",
    "\n",
    "            subProcess.addModel(modelName, hidden_layer_sizes = parameterSet[0], activation = parameterSet[1], max_iter = 1000)\n",
    "            pred = subProcess.useModel(modelName)\n",
    "\n",
    "            # Computing RMSE\n",
    "\n",
    "            allParametersResults[index, j] = compute_rmse(pred, subProcess.Y_test*1e-6)\n",
    "\n",
    "    # Computing wich set of parameters is the best one\n",
    "\n",
    "    meanResults = np.mean(allParametersResults, axis = 1)\n",
    "    bestParams = allParameterSets[np.argmin(meanResults)]\n",
    "    print(f\"\\nBest parameters' values are : {bestParams}\")\n",
    "\n",
    "    # Estimation of the RMSE with optimal parameters\n",
    "\n",
    "    # Preprocessing\n",
    "\n",
    "    # Replacing missing values by the mean\n",
    "    globalProcess.X_train.fillna(globalProcess.X_train[\"runtime\"].mean(), inplace=True)\n",
    "    globalProcess.X_test.fillna(globalProcess.X_train[\"runtime\"].mean(), inplace=True)\n",
    "\n",
    "    # Removing duplicated data from the training set\n",
    "    globalProcess.removeDuplicate(colmns)\n",
    "    \n",
    "    # Selecting the most relevant components of the embeddings with PCA\n",
    "    globalProcess.PCA_embeddings(\"text_embeddings\", 100)\n",
    "    globalProcess.PCA_embeddings(\"img_embeddings\", 100)\n",
    "\n",
    "    # Removing redundant features because of their correlation\n",
    "    #globalProcess.removeRedundantFeatures(threshold = 0.8)\n",
    "\n",
    "    # We scale the data with minmax (we take as min/max the maximal non-outliers values)\n",
    "    globalProcess.minmaxize(withOutliers = False)\n",
    "\n",
    "    # Keeping only the features which are correlated with the target \n",
    "    # Correlation threshold = 0.1\n",
    "    #globalProcess.corrThreshold()\n",
    "    globalProcess.MI_selection(40, worst=False)\n",
    "\n",
    "    globalProcess.addModel(modelName, hidden_layer_sizes = bestParams[0], activation = bestParams[1], max_iter = 1000)\n",
    "    pred = globalProcess.useModel(modelName)\n",
    "    modelResults[i] = compute_rmse(pred, globalProcess.Y_test*1e-6)\n",
    "    print(f\"In these conditions RMSE value on this iteration is : {modelResults[i]}\\n\")\n",
    "\n",
    "meanModelResult = np.mean(modelResults)\n",
    "print(f\"The mean RMSE for the {modelName} model in best conditions is {meanModelResult}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c4b669670de55a09d689e3d4ecca6dc3c283208733e1dbfe915e739bf3f36f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
